\documentclass{article}

\input{utils/def_command.tex}
\input{utils/package_import.tex}

% Page and title formatting
\usepackage[margin=0.78in]{geometry}
\usepackage{titling}

% Compact title formatting
\setlength{\droptitle}{-4em}
\pretitle{\begin{center}\large}
\posttitle{\end{center}\vspace{-1em}}
\preauthor{\begin{center}\normalsize}
\postauthor{\end{center}\vspace{-3em}}
\predate{\begin{center}\normalsize}
\postdate{\end{center}\vspace{-0.5em}}

% Section formatting
\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries}
\titlespacing{\section}{0pt}{1em}{0.5em}

\begin{document}
\title{DM887 Assignment 2: Game AI with Deep Expected SARSA}
\author{Victor Kaplan Kjellerup (vikje24)}
\date{}  % Add this line to prevent date from being printed
\maketitle

\definecolor{dgreen}{rgb}{0,0.75,0}



\section{\(Q\)-network design}
My algorithm and network design builds on the repository code found at https://github.com/iewug/Atari-DQN. For all models, I used the regular DQN from the repository as a starting point, but made it smaller to save computational resources. The input to the model is a stacked tensor consisting of four observations \([\text{obs}_t, \text{obs}_{t-1}, \text{obs}_{t-2}, \text{obs}_{t-3}]\), each being a monochrome frame extended to quadratic size \(84 \times 84\) \(\text{pixels}^2\). The use of four observations provides the network with crucial temporal information.

The network consists of three 2-d convolutional and two fully connected linear layers. All but the last layer uses Relu activation. The convolutional layers have out-channels [16, 24, 32], kernel sizes [8, 4, 3] and strides [4, 2, 1], respectively. This leads to lower-resolution, but richer feature representations for each layer. The deepest convolutional layer has 32 ``latent frames'' of size \(7 \times 7\). The fully connected linear layers have out-channels \([128, |\mathcal{A}|]\), \(\mathcal{A}\) being the action space for the current game. Their role is to distill the rich encoding into 128 latent features, and finally one estimated value for each \(a \in \mathcal{A}\), given the four input frames as its state \(s\).

\section{Description of learning algorithms}
I implemented three learning algorithms: \(Q\)-learning, and Deep Expected SARSA (DES) both with and without weighted importance sampling (WIS), see Algorithm 1.
For all models, I used a replay buffer \(\mathcal{D}\) containing the 25,000 most recent \((s, a, r, s')\) events (larger buffer sizes led to memory issues on my machine). Each game was initially played for 1000 warm-up steps taking random actions to populate the buffer. The game was then played a fixed number of times \(K\). In each step, the policy was first updated with the current \(\eps\) (decreasing exponentially from 1 to 0.05). With probability \(\eps\) the action \(a\) was sampled uniformly, each action with probability \(\frac{1}{|A|}\), and with probability \(1-\eps\) the greedy choice (according to the policy-netwrok \(Q^\pi\)) was chosen. Therefore, \(\pi(a|s) = (1 - \eps) \mathbbm{1}_{\set{a = \arg\max_{a'}\set{Q(s,a')}}} + \frac{\eps}{|A|}\). The reward \(r\) and next state \(s'\) were extracted according to \((s,a)\), the \((s, a, r, p(s|a), s')\)-event pushed to the buffer, and the environment moved to the next state.

The policy-network \(Q^\pi\) was updated based on a batch \(\tilde{\mathcal{D}}\) of 32 samples from the buffer, but differently for each model. For \(Q\)-learning, the Bellman target was calculated using the \(Q\)-value of the best action to take from state \(s'\). For DES, the target value \(\hat{V}(s')\) of the next state was estimated as the expected reward over all possibly actions according to their probabilities. The (weighted) MSE loss \(l\) was computed to update \(Q^\pi\) with Adam optimization.

I use separate networks \(Q^\pi\) and \(Q^-\) for policy-updates and target estimation, respectively (double \(Q\)-network). This diminishes overestimation bias arising when actions are selected and evaluated using the same values (especially for \(Q\)-learning which chases the best option which could be ill-informed by bootstrapping, possibly creating feed-back loops). \(Q^-\) is updated every 1000 steps as a copy of \(Q^\pi\), such that they are different, but not too much.

In DES with WIS, the job of the weights \(w\) is to correct each loss term for the fact that some actions collected under the old policy \(p\) can be over/under-represented in the buffer, compared to what we would have seen under the current policy \(\pi\). If an action was less likely to occur then, we should expect to sample fewer such actions from the buffer, so each example should have larger weight, and vice versa. We cannot sample \(l(a)_{a \sim \pi}\) as desired (other arguments of \(l\) omitted), but we can sample \(l(a)_{a \sim p}\) and device some weight function \(w\) such that \(w l(a)_{a \sim p}\) acts as a good replacement. If unbiasedness were the main concern, I could choose weights \(w' = \frac{\pi(a|s)}{p(a|s)}\), since
\[
    \mathbb{E}_{a \sim \pi}\brks{l(a)}
    = \sum_{a} {\pi(a|s) \ l(a)}
    = \sum_{a} {p(a|s) \ \frac{\pi(a|s)}{p(a|s)} l(a)}
    = \sum_{a} {p(a|s) \ w' \ l(a)}
    % = \mathbb{E}_{a \sim p} \brks{\frac{\pi(a|s)}{p(a|s)} l(a)}
    = \mathbb{E}_{a \sim p} \brks{w' l(a)}.
\]

In some cases, however, when \(\eps=0.05\) in a game where \(|\mathcal{A}| = 10\), say, and \(a\) was not the greedy choice under \(p\), but is under \(\pi\), this fraction could be \(\frac{(1-0.05) + \nicefrac{0.05}{10}}{\nicefrac{0.05}{10}} \ge \frac{0.95}{\nicefrac{0.05}{10}} = 190\), compared to cases with \(\pi \approx p\) where \(w' \approx 1\). To accommodate these extreme events, I accepted some bias by adding \(\delta = 0.1\) in the denominator, yielding \(w := \frac{\pi(a|s)}{p(a|s) + \delta} \le \frac{1}{0.05 + 0.1} \le 7\), limiting variance markedly.

\begin{algorithm}
    \caption{Learning algorithm. Text specific to \textcolor{dgreen}{\(Q\)-learning in green}. Text specific to \textcolor{purple}{Deep Expected SARSA in purple} and \textcolor{blue}{blue when weighted importance sampling is used}.}
    \begin{algorithmic}
        \Require $s_0$ (initial state), $\gamma$ (discount factor)
        \Ensure Updated $Q$ values
        \State \(\mathcal{D} \gets \emptyset\)
        \State Warm-up: collect 1000 examples of $(s, a, p(a|s), r, s') \rightarrow \mathcal{D}$, using $p(a|s) = \frac{1}{|A|} \quad \forall a, s$
        \State Initialize $Q^\pi$, $Q^-$ with weights \(\theta\) randomly
        \For{epoch \(k\) in 1\ldots\(K\)}
            \State $s \gets \text{env.reset()}$
            \While{episode has not ended}
                \State $\pi(\cdot | s) \gets \Call{eps-greedy}{Q_\theta, \eps}$
                \State $a \sim \pi(\cdot|s)$
                \State $p(a|s) \gets (1 - \eps) \cdot \mathbbm{1}_{\set{a = \arg\max_{a'}\set{Q(s,a')}}} + \frac{\eps}{|A|}$
                \State $r, s' \gets \text{env.step}(s, a)$
                \State $\mathcal{D} \cup (s, a, p(a|s), r, s'),\qquad \tilde{\mathcal{D}} \overset{\ita{iid}}{\sim}\mathcal{D}$
                \State $s \gets s'$
                \State $L \gets \emptyset$
                \For{$(s, a, p(a|s), r, s') \in \mathcal{\tilde{D}}$}
                    \State $\textcolor{dgreen}{y \gets r + \gamma \arg\max_{a'}\set{Q^-(s',a')}}$
                    \State $\textcolor{purple}{y \gets r + \gamma \sum\limits_{a' \in \mathcal{A}} \pi(a' | s') Q^-(s', a')}$
                    \State $L \cup \textcolor{blue}{\parr{\frac{\pi(a|s)}{p(a|s) + \delta}}} (Q^\pi(s, a) - y)^2$ %$, \qquad \textcolor{blue}{(\pi(a|s) = (1 - \eps) \cdot \mathbbm{1}_{\set{a\text{ is the greedy choice}}} + \frac{\eps}{|A|})}$
                    \EndFor
                \State Update $Q^\pi$ parameters \(\theta\) using Adaptive Momentum Estimation (Adam) on \(\nabla L\)
                \If{1000 steps have been played since last \(Q^-\)-update}
                    \State $Q^- \gets Q^\pi$
                \EndIf
            \EndWhile
        \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Comparing models}
I trained each model six times for the games Beam Rider, Boxing, and Breakout. During each training, I evaluated the model five times for every ten epochs (games played). Random seeds ensured that all three model types were trained and tested under equal conditions for each experiement. Figure~\ref{fig:results} shows that they all show signs of learning for all the games (although less clearly for boxing, except for the early jump), and that any performance difference between the different model types is not striking. The biggest difference I observe, is that \(Q\)-learning seems to learn Breakout, and possibly Boxing, slightly quicker, while ending with a worse Breakout performance on average. This performance drop is due to one of the six \(Q\)-learning models not reaching the ``breakthrough'' the all other models experienced.

I was initially surprised that \(Q\)-learning and DES performed so equally. However, using a replay buffer, they are both off-policy (on-/off-policy is otherwise their most important difference). The use of a double \(Q\)-network might also remedy one of \(Q\)-learning's main disadvantages, its overestimation bias, making them more equal.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.69\textwidth}
        \includegraphics[width=\textwidth]{figures/beamrider.png}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.69\textwidth}
        \includegraphics[width=\textwidth]{figures/boxing.png}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.69\textwidth}
        \includegraphics[width=\textwidth]{figures/breakout.png}
    \end{subfigure} 
    \caption{Evaluation rewards performed every ten epochs during training for each game and model. The results were first aggregated to the average score \(r_\text{avg}\) for each training run, evaluation round (of five simulations). The average (full line), minimum (lower, dotted line), and maximum (upper, dotted line) of \(r_\text{avg}\) across the six training passes is shown here. In Breakout, the minimum plot for \(Q\)-learning is due to one single training pass essentially not learning the game, affecting the average which was otherwise not significantly worse than its competitors'.}
    \label{fig:results}
\end{figure}

\section{The effects of weighted importance sampling}
As mentioned, any effect of WIS is undetectable from the results. This was initially surprising to me since, theoretically, certain action might be misrepresented in their sampling frequencies. I imagine that my relatively small buffer might explain some of this. Had the buffer been larger, the mismatch between policies \(p\) and \(\pi\) would increase, demanding more importance correction. Entire training passes took between 175,000 and 450,000 steps, so a buffer of 25,000 samples is relatively small, and the unweighted model does still make informed corrections to \(\theta\), although possibly not well-scaled.

\section{Limitations and future considerations}
Due to limited computation time, I stopped all games before convergence (total run time \(\approx50h\)). Although this allowed stronger statistical strength through repetitions, it limits how much I can conclude, since the models might not converge at the same playing level, even though their early learning, which I did report, seems equal.

One possibility I did not explore was to use dueling \(Q\)-networks, in which \(Q(s,a)\) is split into the value \(V(s,a)\) plus the relative advantage \(A(s,a)\) of taking action \(a\), compared to the average. This can emphasize learning of the state values and may be useful when different actions have somewhat similar advantages. In Breakout, for instance, when the ball is far away, or in Boxing, if the two boxers are not within reach of each other, it might not matter what the agent does, because the important decisions come later. Dueling \(Q\)-networks might, however, have complicated the mathematics underlying WIS in ways I have not investigated.

\end{document}
